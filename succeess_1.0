import cv2
import numpy as np
import time
import math
from matplotlib import pyplot as plt
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from mediapipe.tasks.python.vision import GestureRecognizerOptions, GestureRecognizerResult, RunningMode
from mediapipe.framework.formats import landmark_pb2
from mediapipe import solutions

plt.rcParams.update({
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.spines.left': False,
        'axes.spines.bottom': False,
        'xtick.labelbottom': False,
        'xtick.bottom': False,
        'ytick.labelleft': False,
        'ytick.left': False,
        'xtick.labeltop': False,
        'xtick.top': False,
        'ytick.labelright': False,
        'ytick.right': False
    })
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

class HandGestureDectetion:
    def __init__(self):
        self.init_video()
        self.init_mediapipe_detector()

    def init_video(self):
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FPS,30)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH,640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT,640)

    def init_mediapipe_detector(self):
        hand_options = GestureRecognizerOptions(
            base_options=python.BaseOptions(model_asset_path='./gesture_recognizer.task'),
            running_mode=RunningMode.LIVE_STREAM,
            num_hands=2,
            result_callback=self.on_finish_hands
        )
        self.hand_recognizer = vision.GestureRecognizer.create_from_options(hand_options)
        self.hand_result = None
        self.images = None
        # self.finger_index_to_angles = dict()

    def on_finish_hands(self, result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):
        self.hand_result = result
        print(f'hand recognizer result: consume {self.get_cur_time() - timestamp_ms} ms, {self.hand_result}')
        # if len(self.hand_result.hand_world_landmarks) > 0:
        #     self.calculate_all_fingers_angles()

    # def calculate_all_fingers_angles(self):
    #     hand_landmarks = self.hand_result.hand_world_landmarkers[0]
    #     self.finger_index_to_angles.clear()
    #     for i in range(4):
    #         self.finger_index_to_angles[5 + i * 4] = self.calculate_finger_angle()
    #         self.finger_index_to_angles[6 + i * 4] = self.calculate_finger_angle()
    #         self.finger_index_to_angles[7 + i * 4] = self.calculate_finger_angle()
    #         self.finger_index_to_angles[8 + i * 4] = self.calculate_finger_angle()

    def get_cur_time(self):
        return int(time.time() * 1000)

    def run(self):
        while True:
            ret, frame = self.cap.read()
            if ret == True:
                frame2numpy = np.asarray(frame)
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame2numpy)
                self.hand_recognizer.recognize_async(mp_image, self.get_cur_time())

                if self.hand_result:
                    frame = self.display_batch_of_images_with_gestures_and_hand_landmarks(frame, self.hand_result)

                cv2.imshow('frame',frame)
                key = cv2.waitKey(1)
                if key == ord('q'):
                    break
        self.exit()

    def display_one_image(self, image, title, subplot, titlesize=16):
        """Displays one image along with the predicted category name and score."""
        plt.subplot(*subplot)
        plt.imshow(image)
        if len(title) > 0:
            plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment': 'center'},
                      pad=int(titlesize / 1.5))
        return (subplot[0], subplot[1], subplot[2] + 1)

    def display_batch_of_images_with_gestures_and_hand_landmarks(self, images, results):
        """Displays a batch of images with the gesture category and its score along with the hand landmarks."""
        # Images and labels.
        images = [image.view() for image in images]
        gestures = [top_gesture for (top_gesture, _) in results]
        multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]

        # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.
        rows = int(math.sqrt(len(images)))
        cols = len(images) // rows

        # Size and spacing.
        FIGSIZE = 13.0
        SPACING = 0.1
        subplot = (rows, cols, 1)
        if rows < cols:
            plt.figure(figsize=(FIGSIZE, FIGSIZE / cols * rows))
        else:
            plt.figure(figsize=(FIGSIZE / rows * cols, FIGSIZE))

        # Display gestures and hand landmarks.
        for i, (image, gestures) in enumerate(zip(images[:rows * cols], gestures[:rows * cols])):
            title = f"{gestures.category_name} ({gestures.score:.2f})"
            dynamic_titlesize = FIGSIZE * SPACING / max(rows, cols) * 40 + 3
            annotated_image = image.copy()

            for hand_landmarks in multi_hand_landmarks_list[i]:
                hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
                hand_landmarks_proto.landmark.extend([
                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in
                    hand_landmarks
                ])

                mp_drawing.draw_landmarks(
                    annotated_image,
                    hand_landmarks_proto,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style())

            subplot = self.display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)

        # Layout.
        plt.tight_layout()
        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)
        plt.show()

    def exit(self):
        self.cap.release()
        cv2.destroyAllWindows()

